# State-of-the-Art in Multi-Agent LLM Systems and Research Opportunities for IEEE Conference Papers

The field of multi-agent LLM systems has matured significantly in 2024-2025, transitioning from experimental autonomous agents to production-ready frameworks deployed by major enterprises including LinkedIn, Uber, and Klarna. **However, fundamental limitations persist across coordination, context management, and reasoning capabilities—creating substantial opportunities for algorithmic and theoretical contributions achievable with limited compute resources**. Current systems achieve only 33.3% correctness on software development tasks, with 86.7% failure rates on cross-domain applications, while theoretical frameworks for multi-agent coordination, optimal task decomposition, and provable compression guarantees remain absent from the literature.

The most promising research directions for a semester project with 6GB VRAM lie at the intersection of theoretical analysis and practical algorithm design: **establishing formal communication complexity bounds for multi-agent coordination, developing adaptive context compression with provable guarantees, and designing error-resilient task decomposition under uncertainty**. These gaps require minimal computational resources while addressing fundamental theoretical voids that current empirical approaches have overlooked.

## Multi-agent orchestration frameworks have converged on three dominant paradigms with distinct tradeoffs

The 2024-2025 landscape features three production-ready frameworks that represent fundamentally different architectural philosophies. **LangGraph** employs graph-based state machines inspired by Google Pregel, providing explicit control over state transitions with checkpointing and time-travel debugging capabilities. This approach scales gracefully and eliminates abstraction overhead but imposes the steepest learning curve. **Microsoft AutoGen** implements conversation-based orchestration with two-tier architecture (Core + AgentChat), enabling flexible multi-agent patterns including group chats with dynamic speaker selection and secure Docker-based code execution. **CrewAI** adopts role-based organizational models with specialized agents functioning as team members, offering the fastest path to production through clear, intuitive abstractions built atop LangChain.

Performance benchmarks reveal sobering realities beneath architectural sophistication. ChatDev achieves merely 33.3% correctness on ProgramDev tasks, while AppWorld shows 86.7% failure rates on cross-application scenarios. Multi-agent systems frequently demonstrate minimal performance gains over single-agent architectures despite increased complexity. The MAST (Multi-Agent System Failure) Taxonomy identifies 14 unique failure modes across 7 popular frameworks spanning 200+ tasks. Cost remains prohibitive—AutoGPT's iterative loops can consume $14.40 for a single 50-step task at current API pricing.

Critical unsolved challenges cluster around five domains. **Context and memory management** faces limitations where context windows prevent comprehensive state tracking, leading to memory drift and error amplification across agent chains. **Coordination complexity** lacks optimal task allocation strategies, with no principled methods for dynamic replanning when unexpected issues arise. **Scalability** encounters quadratic communication overhead growth as resource requirements balloon with agent count. **Reliability** struggles with hallucination propagation through agent networks and adversarial vulnerabilities including prompt injection and implicit collusion. **Production deployment** confronts fundamental tradeoffs between determinism and autonomy, making emergent system behavior difficult to reason about.

Recent NeurIPS 2024 papers propose several innovations. COPPER introduces counterfactual PPO mechanisms for reflective multi-agent collaboration with improved credit assignment. MacNet explores network-style organizations with dynamic agent selection and optimizable prompt refinement graphs. The most comprehensive recent survey (Tran et al., January 2025) establishes an extensible framework characterizing collaboration via five dimensions: actors, types (cooperation/competition/coopetition), structures (centralized/decentralized/hierarchical), strategies (role-based/rule-based/model-based), and coordination protocols.

The field identifies **artificial collective intelligence** as the paramount open problem. Current systems lack unified governance mechanisms for optimal coordination, struggle with shared decision-making beyond simple voting, and face fundamental limitations as LLMs weren't designed for multi-participant collaboration. Understanding scaling laws for multi-agent systems remains elusive, with no established theory for when emergent generalization might arise. **This represents a high-value research opportunity**: establishing information-theoretic bounds on communication complexity for multi-agent coordination tasks would provide foundational theory currently absent from literature while requiring minimal computational resources for validation on synthetic coordination problems.

## Context window expansion masks persistent working memory limitations

Context windows have exploded from 4K-32K tokens in 2023 to 128K-2M tokens in production models by November 2025, with Gemini 1.5 Pro supporting 2M tokens, Claude 3.5 offering 1M tokens selectively, and GPT-4.1 handling ~1M tokens. Open-source models have followed: Qwen2.5-7B/14B-1M, LLaMA-3-8B-1M, and GLM-4-9B-1M all achieve million-token capabilities. However, **benchmark results reveal effective working memory remains limited to approximately 5-10 variables despite million-token windows**. Models consistently fail bandwidth-dependent tasks that require tracking multiple concepts before exhausting raw context capacity—the "lost in the middle" problem persists even at extreme lengths.

NVIDIA's RULER benchmark (April 2024) demonstrates this gap starkly. Despite perfect scores on simple Needle-in-a-Haystack retrieval, all tested models degrade significantly on RULER's 13 tasks spanning retrieval variants, multi-hop tracing, aggregation, and question answering. Only 4 of 10 models effectively handle their claimed 32K context length, with most failing before reaching stated limits. Top performers GPT-4, Command-R, Yi-34B, and Mixtral still show substantial degradation. LongICLBench (April 2024) reveals models struggle with challenging in-context learning tasks spanning 2K-50K tokens, exhibiting bias toward later labels in long sequences.

**Context compression techniques** have emerged as the primary workaround. LongLLMLingua (ACL 2024) achieves 4x-6x compression with 1.4x-2.6x latency reduction, delivering 21.4% performance improvements on NaturalQuestions with 94% cost reduction on LooGLE benchmarks through perplexity-based token importance scoring. IC-Former (EMNLP 2024) provides the fastest compression at 68-112x speedup with only 1/32 baseline FLOPs, using cross-attention with learnable digest tokens achieving linear O(n) complexity. However, both methods lack theoretical analysis of information preservation guarantees and employ fixed compression ratios regardless of content characteristics.

The breakthrough in attention mechanisms comes from **sparse attention patterns**. MInference 1.0 (NeurIPS 2024, Microsoft) demonstrates that 95% of attention computation is unnecessary, achieving 10x speedup for 1M token contexts on single A100 GPUs (30 minutes → 3 minutes) while maintaining or improving accuracy. The system identifies three key patterns: A-shape (initial tokens + local windows), Vertical-Slash (specific tokens + fixed intervals), and Block-Sparse (distributed clusters). Kernel-aware pattern search dynamically assigns optimal patterns per attention head based on input characteristics. Alternative approaches include linear attention mechanisms (Lightning Attention, Mamba state-space models) and hierarchical methods (Hierarchical Context Merging, MoICE mixture of in-context experts).

**RAG has evolved beyond simple retrieval** into sophisticated multi-method approaches. GraphRAG (Microsoft, mid-2024) employs knowledge graphs with hierarchical pre-clustering to address semantic gaps in traditional retrieval. RAPTOR implements recursive summarization preserving multi-document reasoning context. BlendedRAG (IBM Research, April 2024) combines dense, sparse, and graph-based retrieval for superior cross-task performance. Long RAG works with larger retrieval units (document sections rather than small chunks) to reduce fragmentation while preserving narrative context.

Memory mechanisms represent another frontier. **EM-LLM** (July 2024, updated October 2025) implements human-inspired episodic memory supporting infinite context through Bayesian surprise detection and graph-theoretic boundary refinement, demonstrating retrieval across 10M tokens without fine-tuning while outperforming RAG and InfLLM on LongBench/∞-Bench. Larimar (March 2024) employs brain-inspired distributed episodic memory enabling one-shot knowledge updates with 8-10x speedup, incorporating mechanisms for selective forgetting and leakage prevention. MemGPT treats context windows as constrained memory resources with hierarchical management analogous to operating systems.

**Critical unsolved problems** cluster around three areas. Working memory bandwidth remains the primary bottleneck—tasks requiring sustained reasoning over long contexts fail despite sufficient raw capacity. Long-form generation (>10K token outputs) remains significantly under-explored compared to long-context inputs, with coherence degradation in extended generation. Multi-hop reasoning at scale across 100K+ tokens with complex logical chains maintains poor reliability. Dynamic context understanding with real-time updates and changing semantics lacks robust solutions.

**This presents a clear research opportunity**: developing adaptive compression algorithms with provable information-theoretic bounds would address a fundamental gap in current literature. Existing methods (IC-Former, RCC, LongLLMLingua) employ fixed compression ratios and lack formal analysis of compression-accuracy tradeoffs. A theoretical framework deriving bounds combined with practical content-aware compression algorithms could be validated using small models (GPT-2, DistilBERT) within 6GB VRAM constraints, offering both theoretical contribution and practical impact.

## Task decomposition and planning reveal fundamental reasoning limitations

Current approaches to task decomposition span pure neural methods through hybrid neuro-symbolic architectures. **ADaPT** (Prasad et al., NAACL 2024) implements as-needed recursive decomposition that activates only when LLM executors fail, achieving 28.3% improvement on ALFWorld, 27% on WebShop, and 33% on TextCraft by adapting decomposition depth to both task complexity and LLM capability. Hierarchical multi-agent systems like AgentOrchestra and Agent-E employ planning agents for high-level reasoning with specialized sub-agents for execution, with Agent-E achieving 73.2% success on WebVoyager (20% improvement over previous SOTA). Neuro-symbolic approaches (Kwon et al., 2024) combine LLM task decomposition with symbolic planners (PDDL) or Monte Carlo Tree Search for subgoal planning.

**Chain-of-Thought variations** have proliferated with mixed results. Tree-of-Thoughts achieves 74% success on Game of 24 (49% improvement over CoT) and 20% win rates on crosswords versus 1% for standard CoT, but at extreme resource costs unsuitable for most applications. Graph-of-Thoughts extends beyond linear reasoning chains to arbitrary graph structures, reaching 87.59% accuracy on ScienceQA versus 85.19% baselines. Adaptive Graph-of-Thoughts (2025) provides test-time adaptive reasoning with +46.2% improvement on GPQA over baselines, matching RL-based training methods without training overhead.

Meta-reasoning capabilities show promise but high variability. Reflexion (Shinn et al., 2023) implements self-critique and iterative improvement. Meta-Rewarding (2024) enables models to judge their own judgments, improving Llama-3-8B from 22.9% to 39.4% win rate. DeepSeek-R1 (Nature, 2025) demonstrates emergent self-reflection and verification through pure RL training without explicit prompting. However, self-recognition accuracy ranges from 8-80% across models, self-correction frequently flips correct answers to incorrect ones, and effective meta-reasoning typically requires explicit prompting rather than arising naturally.

**Fundamental limitations** undermine current planning capabilities. Kambhampati's ACL 2024 analysis demonstrates LLMs struggle with basic planning, cannot reliably compose reasoning chains, and face a "globality barrier" preventing true compositional reasoning. Apple Research's "The Illusion of Thinking" (2024) characterizes AI reasoning as brittle façade—statistical pattern matching that resembles but differs fundamentally from genuine planning. TravelPlanner benchmark exposes this gap dramatically: pure GPT-4 achieves merely 0.6% success rate on travel planning queries, while integration with SMT solvers reaches 97% success—a 160x improvement demonstrating the power of hybrid neuro-symbolic approaches.

Specific bottlenecks plague production deployments. Context window limitations prevent maintaining complete task history, causing plan drift to accelerate over time. Dependency management failures lead to underestimated dependencies causing delays and cascading failures. Error propagation compounds early mistakes downstream with difficult root-cause analysis. Resource constraints multiply costs through multiple LLM calls while latency increases with decomposition depth. Coordination overhead in multi-agent synchronization consumes thousands of cycles. Amazon Science and AI21 research identifies decomposition challenges: increased system complexity offsets cost savings, over-decomposition yields diminishing returns, loss of holistic creativity, documentation difficulties, and compliance risks in regulated industries.

**Formal methods integration** represents the most promising direction. VeriPlan (CHI 2025) combines rule translators, flexibility sliders, and model checkers to achieve 96.3% F1 score on simplified PlanBench, improving perceived quality, control, and transparency. Research bridging LLM planning agents with formal methods converts natural language plans to Kripke structures and Linear Temporal Logic specifications, enabling automated model checking with NuSMV—GPT-5 achieves 96.3% F1 with syntactically perfect formal representations, though semantic perfection remains challenging. The Fusion of Large Language Models and Formal Methods roadmap (2024) outlines comprehensive integration of SMT solvers, model checking, and theorem proving for trustworthy AI agents.

**Key open problems** include determining when to decompose (no principled methods for optimal decomposition levels), failure recovery (graceful degradation mechanisms lacking), long-horizon credit assignment (attributing failures in extended chains), multi-objective optimization (balancing cost, time, quality), and provable guarantees (formal verification integration remains early-stage). **This suggests a valuable research direction**: probabilistic task decomposition accounting for sub-task failure probabilities and error propagation could provide robust decomposition under uncertainty. Current methods (ADaPT, TDAG) assume deterministic execution—modeling tasks as probabilistic graphs with failure-aware decomposition strategies would fill this gap with primarily algorithmic contributions validatable on toy environments within compute constraints.

## Small language models excel at narrow tasks but fail at complex reasoning

Models in the 1-2B parameter range have achieved substantial progress through improved training data and distillation techniques. **Qwen2-1.5B** leads the sub-2B category with 52.4% MMLU, 29.9% HumanEval, and 40.1% GSM8K—best overall performance under 2B parameters as of November 2025. Phi-3-mini (3.8B, slightly above range) achieves 69% MMLU with 100% success on some enterprise benchmarks, effectively performing like 7B models when quantized to 2.4GB through training on 3.4T tokens of high-quality, reasoning-rich synthetic data. Gemma 2B (Google, 2024) reaches ~56% MMLU through knowledge distillation from Gemini models across 2-3 trillion training tokens. Llama 3.2-1B optimizes for on-device deployment with 128K context window. SmolLM-1.7B achieves 50.3% average accuracy as best performer under 2B at release, trained on FineWeb-Edu, StarCoder, and Cosmopedia datasets.

**Reliable capabilities** concentrate on well-defined, single-step tasks. Small models handle simple text generation, basic summarization of single documents, sentiment analysis (binary and multi-class), named entity recognition for standard entities, factual question answering, code completion for common patterns, and classification tasks (category, intent, sentiment) with 94% accuracy on boolean questions in 2024 versus 90% in 2023. Short-form content generation for social media posts and simple emails performs adequately.

**Multi-step reasoning** exposes the "Small Model Learnability Gap"—1-2B models struggle to benefit from long Chain-of-Thought reasoning unlike their larger counterparts. GSM8K performance typically scores 40-50% for 1.5B models versus 80%+ for 7B+ models, with significant gaps in complex logic and formal reasoning. Multi-digit arithmetic without external tools consistently fails. **Tool calling represents a critical weakness**: off-the-shelf small models possess "very low function calling capabilities" requiring specialized fine-tuning. Berkeley's TinyAgent framework (2024) achieves 80.06% success rate with 1.1B models only after specialized training with high-quality synthetic data generation and ToolRAG for efficient tool retrieval—still below GPT-4 performance. Parallel function calling, multi-turn tool interactions, and complex API parameter extraction remain problematic.

**Instruction following** shows inconsistent performance on IFEval benchmarks, with significant failures on complex multi-constraint instructions. Models like Qwen2.5-0.5B-Instruct optimized for instructions perform better but still lag on complex constraints, missing implicit requirements, failing to maintain multiple constraints simultaneously, struggling with conditional instructions, and having difficulty with negations and edge cases. Common failure modes include probabilistic pattern-matching rather than true logical reasoning, high distribution shift sensitivity to prompt variations, lack of common-sense inference for unstated assumptions, more frequent hallucinations than larger models especially in knowledge-intensive tasks, degraded performance beyond 2K tokens, consistently poor spatial reasoning, and inability to maintain multi-step plans effectively.

**Domain-specific specialization** offers the clearest path to competitive performance. BioGPT (335M parameters) outperforms few-shot GPT-4 on PubMedQA through focused medical training. Code Llama Python 7B exceeds Llama 2 70B on specific coding tasks. Stable Code 3B matches Code Llama 7B on Multi-PL benchmarks. WizardMath 7B surpasses most 7-40B models on GSM8K through specialized multi-step reasoning training. FLAME (60M parameters) outperforms Codex-175B on Excel formula generation in specific settings. SLaDe (200M) exceeds ChatGPT on assembly-to-C decompilation. These successes share common elements: narrow task focus, specialized training corpora, task-specific fine-tuning, and often tool integration.

**Distillation techniques** have advanced significantly. Step-by-step distillation breaking complex reasoning into sub-questions achieves 70%+ performance improvements on reasoning datasets, proving more effective for small models than full CoT. DistillKit (Arcee AI, 2024) provides open-source logit-based and hidden-state distillation enabling Qwen2-1.5B distilled from Qwen-7B to achieve significant gains. Decompositional distillation allows 3B models to outperform 11B baselines. SOCRATIC CoT enables GPT-2 Large to exceed GPT-3 6B performance. Microsoft's December 2024 distillation with enable_chain_of_thought parameter for reasoning-based training supports both cloud (GPT-4o mini) and local (Llama) deployment.

**Benchmarks reveal persistent gaps**. MMLU shows ~34 point spread between Qwen2-1.5B (52.4%) and GPT-4 (86.4%). HumanEval demonstrates ~62 point gap between small models (~30%) and Claude 3.5 Sonnet (92%). GSM8K shows small models can compete with distillation (TinyGSM-enhanced 1.3B reaches 81.5% versus GPT-4's 92%), but this requires specialized training. General performance improvements from 2023 to 2024 include +10.4% on commonsense reasoning, +13.5% on problem-solving, math/logic improvement from 71% to 83%, and boolean accuracy increase from 90% to 94%.

**VRAM feasibility analysis** for 6GB constraints shows 1-2B models fit comfortably in FP16 (~2-3GB model + 2-3GB context/activations), 5-6B models work with Int8 quantization, and up to 10-12B models possible with Int4 quantization. Optimal models for RTX 4050 6GB include Qwen2-1.5B FP16 (~3GB total), Phi-3-mini quantized (~2.4GB), Gemma 2B FP16 (~4GB), TinyLlama-1.1B FP16 (~2.2GB), and Mistral 7B Int4 (~3.5GB). Inference speeds range from 20-50 tokens/second for 1B models to 5-15 tokens/second for 7B Int4 models.

**Practical tasks feasible on 6GB VRAM** with excellent performance include text classification (90%+ accuracy), named entity recognition (85%+ F1), simple Q&A, content moderation, basic summarization (<2K tokens), code completion, translation with fine-tuning, simple chatbots, and data extraction. Good performance with optimization includes RAG systems with external knowledge bases, simple function calling with well-defined APIs, basic instruction following, math with calculator tools, simple code generation, and document Q&A with chunking. Consistently poor performance occurs with complex multi-step reasoning, advanced math, long documents (>4K tokens), creative writing, multi-turn tool use, complex instructions, spatial reasoning, strategic planning, causal reasoning, and ambiguity resolution.

## Research gaps cluster around theoretical foundations and algorithmic efficiency

**Communication complexity** for multi-agent coordination lacks formal frameworks entirely. Tran et al.'s comprehensive January 2025 survey identifies absence of theoretical frameworks for coordination complexity, no established scaling laws for multi-agent systems, underexplored communication protocol optimization, and limited work on hybrid collaboration structures. Current systems employ ad-hoc, domain-specific protocols without general-purpose, provably efficient alternatives. Guo et al. (IJCAI 2024) emphasizes scalability challenges where resource requirements scale poorly with agent count, failure to capitalize on collective intelligence synergies, hallucination propagation cascades, lack of systematic orchestration frameworks for large-scale coordination, and evaluation gaps for emergent multi-agent behaviors.

NeurIPS 2024's SeqComm (Ding et al.) demonstrates two-phase communication (negotiation + launching) with asynchronous agent priorities but explicitly states coordination problems persist due to circular dependencies, priority assignment remains heuristic rather than optimal, and scalability deteriorates beyond 10-15 agents. COPPER (NeurIPS 2024) applies counterfactual PPO for self-reflection but requires extensive training data, depends on LLM size for reflection quality, and lacks theoretical convergence guarantees. **This represents the highest-priority research gap**: establishing information-theoretic bounds on communication requirements versus task complexity with algorithmic protocols would provide foundational theory with purely algorithmic/theoretical work requiring minimal compute, validatable on small synthetic environments, offering high novelty as no existing framework exists.

**Context compression** advances lack theoretical foundations. IC-Former (Wang et al., EMNLP 2024) achieves 68-112x faster compression with 1/32 FLOPs and >90% baseline performance through cross-attention mechanisms with learnable digest tokens, but states limitations of effective 4x-8x compression ratios with performance degradation on very long contexts (>8K tokens) requiring task-specific fine-tuning. Recurrent Context Compression (ICLR 2025 submission) reaches 32x compression but acknowledges degraded performance when both instructions and context compressed, no theoretical guarantees on information preservation, and sequential processing bottlenecks. In-context Autoencoder requires extensive pretraining with fixed memory slots and quadratic complexity during training.

Identified research opportunities include theoretical frameworks for compression bounds (no formal analysis of information-theoretic limits—deriving compression-performance tradeoff bounds with high feasibility through theoretical work), adaptive compression algorithms (current fixed ratios versus content-aware dynamic compression with high feasibility testing on small models), and streaming compression for real-time agents (current batch-only processing versus online compression with provable guarantees at medium feasibility). **Recommended project**: adaptive context compression with provable guarantees would formalize compression as constrained optimization, derive information-theoretic bounds on compression-accuracy tradeoffs, design adaptive algorithms adjusting compression based on content, prove approximation guarantees, and validate using GPT-2 or DistilGPT-2 on QA datasets (SQuAD, Natural Questions) focusing on 1K-4K context lengths feasible within 6GB VRAM.

**Task decomposition** reveals theory-practice gaps. ADaPT (Prasad et al., NAACL 2024) improves performance 28.3% on ALFWorld but acknowledges decomposition depth not optimized, high LLM call counts creating expense, and no formal guarantees on completeness. TDAG (Wang et al., January 2025) suffers error propagation in decomposition cascades, limited adaptability to novel task types, and absence of theoretical frameworks for decomposition quality. AgentGroupChat-V2 lacks domain generality with no adaptation to task complexity. The fusion of formal methods and LLMs shows promise—VeriPlan achieves 96.3% F1 but converting natural language to formal specifications faces semantic challenges despite syntactic success.

Open problems include determining when to decompose (no principled methods for optimal levels), failure recovery (graceful degradation mechanisms lacking), long-horizon credit assignment (difficult attribution in long chains), multi-objective optimization (balancing cost/time/quality), human-AI collaboration interfaces, provable guarantees (verification integration early-stage), resource allocation (optimal compute distribution), transfer learning (strategies don't transfer across domains), interpretability (understanding success/failure), and adaptive granularity (dynamic adjustment based on progress). **Recommended project**: probabilistic task decomposition under uncertainty would model tasks as probabilistic graphs with failure probabilities, design robust decomposition minimizing expected failure, analyze error propagation, and validate experimentally on TextWorld/ALFWorld with small LLMs or rule-based executors focusing on algorithmic contributions.

**Memory organization** lacks dynamic adaptation. A-MEM (Xu et al., February 2025) implements Zettelkasten-inspired interconnected networks achieving 2x better performance on Multi-Hop tasks versus LoCoMo/MemGPT but employs fixed memory structure after initialization with no theoretical analysis and unoptimized retrieval efficiency. MemoryOS (Kang et al., May 2025) establishes STM→MTM→LTM hierarchy with FIFO and heat scoring but uses fixed queue sizes (100 items), simple decay functions, and no adaptive capacity management. Research opportunities include theoretical memory organization (formal analysis of optimal structures through graph theory + small-scale experiments), adaptive memory compression (dynamic compression based on importance and age), and memory consistency algorithms (resolving contradictions with provable properties through algorithmic work + toy validation).

**Evaluation frameworks** show critical gaps. Survey on Evaluation of LLM-based Agents (March 2025) identifies lacking safety evaluation frameworks, needed fine-grained evaluation metrics, and inadequate cost-efficiency measurement. "Agents Are Not Enough" (December 2024) highlights hallucination propagation in agent chains, complexity handling in long multi-step tasks, and safety/ethical risks with autonomy requiring verification and safety mechanisms.

## Recent theoretical and algorithmic contributions emphasize efficiency over scale

Papers from top venues increasingly focus on algorithmic improvements rather than pure scaling. **NeurIPS 2024** featured multiple theoretical contributions: "Large Language Models as Commonsense Knowledge for Large-Scale Task Planning" integrated LLMs with Monte Carlo Tree Search, "Leveraging Environment Interaction for Automated PDDL Generation" enabled automated formal specification creation, "A*-Thought: Efficient Reasoning via Bidirectional Compression" applied classic search algorithms to reasoning, and "Unveiling Causal Reasoning in Large Language Models" analyzed fundamental reasoning capabilities. Workshop proceedings emphasized need for formal coordination frameworks and sim-to-real transfer challenges.

**ICLR 2025** submissions include Recurrent Context Compression achieving 32x compression through instruction reconstruction methodology, and multiple papers on hierarchical context merging for better long-context understanding in pre-trained LLMs. **ACL/EMNLP 2024** contributed LongLLMLingua's prompt compression with 4x-6x token reduction maintaining performance, IC-Former's 68-112x faster compression enabling real-time feasibility, and work on episodic memory retrieval implementing neuromorphic mechanisms for counterfactual generation. **ICML 2024** advanced Quest's query-aware sparsity for efficient long-context inference and DAPE's data-adaptive positional encoding for improved length extrapolation.

Notable 2024-2025 arXiv papers include "From 128K to 4M" (April 2025) on efficient training of ultra-long context LLMs with practical scaling strategies, "Native Sparse Attention" (February 2025) on hardware-aligned training, EM-LLM (updated October 2025) on human-inspired episodic memory with 10M token retrieval capability requiring no fine-tuning, and ByteScale (February 2025) on efficient scaling to 2048K context on 12,000+ GPUs. Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning (2024) provided first comprehensive review of Meta-RL + multi-agent systems for introspection.

**Formal methods integration** represents the strongest algorithmic trend. VeriPlan (CHI 2025) integrates rule translators, flexibility sliders, and model checkers achieving 96.3% F1 score on PlanBench with improved perceived quality, control, and transparency. "Large Language Models Can Solve Real-World Planning Rigorously" (2024) formalizes planning as SMT problems with interactive constraint solving, achieving 97% success on TravelPlanner versus 0.6% for pure LLM—demonstrating 160x improvement through formalization with strong zero-shot generalizability. "Bridging LLM Planning Agents and Formal Methods" (2024) converts natural language plans to Kripke structures and Linear Temporal Logic specifications enabling automated model checking with NuSMV, with GPT-5 achieving 96.3% F1 and syntactically perfect formal representations.

Theoretical frameworks emerging include Minimum Description Length guiding when decomposition benefits, Dual-Process Theory distinguishing fast thinking (direct LLM inference) from slow thinking (structured reasoning with verification), Model Checking using Kripke structures + Linear Temporal Logic specifications, and Satisfiability Modulo Theories encoding constraints as logical formulas solved by Z3/CVC5. Position papers explicitly identifying open problems include "Towards Responsible LLM-empowered Multi-Agent Systems" (February 2025) proposing trust scores, consensus mechanisms, and human-centered design addressing adversarial challenges, and "LLM Multi-Agent Systems: Challenges and Open Problems" (Han et al., February 2024) emphasizing task allocation optimization, iterative debate for robust reasoning, and complex context/memory management.

## Concrete recommendations for semester projects on limited hardware

Your RTX 4050 with 6GB VRAM enables three high-value research directions combining theoretical contributions with practical validation. **First recommendation**: Establishing information-theoretic bounds for multi-agent communication would formalize coordination as distributed optimization, derive lower bounds using information theory, design optimal or near-optimal communication protocols, and validate on synthetic coordination tasks. This pure algorithmic/theoretical work requires minimal compute (synthetic agents, toy environments), addresses fundamental gaps (no existing framework), offers high novelty, fits IEEE conferences perfectly, and achieves completion within one semester. Timeline: weeks 1-4 literature review + theoretical framework, weeks 5-8 algorithm design + formal proofs, weeks 9-12 implementation + experiments, weeks 13-16 paper writing.

**Second recommendation**: Adaptive context compression with provable guarantees would formalize compression as constrained optimization, derive information-theoretic bounds on compression-accuracy tradeoffs, design adaptive algorithms adjusting compression ratios based on content importance, prove approximation guarantees, and evaluate empirically. This extends IC-Former and RCC by adding missing theoretical analysis while remaining feasible with 6GB VRAM using GPT-2 or DistilGPT-2 on QA datasets (SQuAD, Natural Questions) focusing on 1K-4K context lengths. The combination of theory and systems perfectly suits IEEE venues. Timeline: weeks 1-4 formalization + theoretical analysis, weeks 5-9 algorithm design + proofs, weeks 10-14 implementation + experiments, weeks 15-16 paper writing.

**Third recommendation**: Probabilistic task decomposition under uncertainty would model tasks as probabilistic graphs (nodes = sub-tasks, edges = dependencies) with failure probabilities, design robust decomposition minimizing expected failure, analyze error propagation formally, and validate experimentally. This addresses ADaPT and TDAG's limitation of not accounting for sub-task failure probabilities. Implementation uses rule-based environments (TextWorld, ALFWorld) with small LLMs (GPT-2) or rule-based executors focusing on algorithmic contributions. High feasibility and high novelty combine with clear validation paths. Timeline: weeks 1-3 problem formulation, weeks 4-8 algorithm design, weeks 9-14 implementation + experiments, weeks 15-16 paper writing.

**Critical success factors** include focusing on algorithms rather than scale (your compute constraint becomes an asset designing efficient methods), adding theoretical analysis (current work lacks formal frameworks), small-scale validation (proof of concept with toy problems suffices), combining topics (e.g., "Communication-Efficient Context Compression for Multi-Agent Systems"), and emphasizing efficiency (6GB limitation motivates designing resource-efficient methods). Avoid projects requiring large-scale multi-agent experiments with many large LLMs simultaneously, heavy empirical comparisons across numerous benchmarks, training large models from scratch, or real-world deployments (too ambitious for semester scope).

**Venue recommendations** include top-tier IEEE conferences: ICRA (Robotics and Automation) for multi-agent/embodied angles, IROS similarly, ICASSP for signal processing/compression angles, ICWS (Web Services) for agent coordination/services angles, IEEE ICMLA (Machine Learning and Applications), IEEE BigData, and IEEE ICDM (Data Mining). Consider non-IEEE venues including AAMAS (Autonomous Agents and Multi-Agent Systems) as perfect fit, ICLR/NeurIPS/ICML workshops for preliminary work, and ACL/EMNLP workshops for NLP angles.

## The path forward prioritizes theory-driven efficiency over empirical scale

The field's maturation from experimental systems to production deployments has exposed fundamental gaps in theoretical foundations. While frameworks like LangGraph, AutoGen, and CrewAI enable practical multi-agent applications, and context windows have exploded to millions of tokens, persistent failures in coordination, compression guarantees, optimal decomposition, and robust planning reveal the limits of purely empirical engineering. Performance gaps—33.3% correctness on software development, 0.6% success on travel planning without formal methods, 86.7% failure rates cross-domain—demonstrate that scale alone cannot overcome algorithmic deficiencies.

Your semester project with 6GB VRAM positions you ideally to address these gaps. Small models (1-2B parameters) provide sufficient capability for validation while forcing focus on efficiency—a constraint that becomes an asset. The strongest opportunities lie where current systems show weakest theoretical foundations: communication complexity lacking formal bounds, context compression without provable guarantees, task decomposition ignoring uncertainty, and memory organization without principled adaptation. These problems require algorithmic innovation and theoretical analysis, not massive compute resources.

The most promising direction combines multiple aspects: design communication-efficient algorithms for multi-agent coordination with formal complexity analysis, validate using context compression as the communication medium, test on task decomposition problems requiring coordination under uncertainty, and demonstrate feasibility with small models on toy environments. This integration addresses multiple gaps simultaneously while remaining achievable in one semester, offers both theoretical contributions (formal frameworks, proofs) and practical validation (working implementations), and aligns perfectly with IEEE conference expectations for algorithmic rigor combined with empirical evidence. The field needs theoretical foundations—your compute constraints position you to provide them.